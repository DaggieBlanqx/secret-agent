{
    "version": "https://jsonfeed.org/version/1",
    "title": "SecretAgent Blog",
    "home_page_url": "https://secretagent.dev/",
    "feed_url": "https://secretagent.dev/feed.json",
    "description": "A blog about scraping, features and experiences developing SecretAgent",
    "items": [
        {
            "id": "https://secretagent.dev/blog/chromium-chrome",
            "content_html": "\nWe're moving our underlying engine from Chromium to Chrome in the coming weeks. \n\n## Why?\nThere are a few reasons we decided to go this direction:\n1. Chrome is the actual browser being used in the wild by consumers.\n2. Chrome has increasingly diverged from Chromium. In our DoubleAgent testing, we're seeing Chrome 85-89 steadily diverge features. This makes it harder and harder to emulate Chrome when using Chromium as the engine.\n3. Chrome has certain features that aren't in Chromium that will be nearly impossible to emulate (x-headers to Google sites, Widevine, etc). In theory, you could use DRM as a way to weed out Chromium users masking themselves as Chrome users.\n\n## Chrome Version-Specific Installers \nThis switch was somewhat challenging, primarily because the Chrome team doesn't openly publish versions of Chrome that stay on the version you want them on. Even on Ubuntu, if you install the .deb release, it will install an apt updater, and if you're not careful, your engine will swap out underneath you.\n\nOur first task was to go out and find stable Chrome installations for each version. We created a new project called [chrome-versions](https://github.com/ulixee/chrome-versions) that downloads, extracts, and stores versions of Chrome for Linux, Windows and Mac. \n\nFor each version, we stripped out the auto-update features and converted them to .tar archives that can be extracted side-by-side. They're then published on Github as release assets for each Chrome version (eg, https://github.com/ulixee/chrome-versions/releases/tag/88.0.4324.182)\n\nOn Debian/Ubuntu, Chrome often needs packages to be installed. We re-bundled the .deb control file into a new installer that can be run after you extract the chrome executable - this makes setting up CI or docker very simple for 1 or more Chrome installations.\n\n## SecretAgent\nSecretAgent has been updated to use Chrome everywhere. Our emulators have \"polyfills\" auto-generated for how to resemble Chrome headed when running each version headless. The changes are significant enough from Chromium that you need to actually use Chrome underneath. \n\nNo changes should be visible in your scripts, but you might see some installation changes as you go to upgrade. We also experienced some changes in no-sandbox features when running on Docker. Your mileage here may vary. \n\nThis new release will have an updated Dockerfile and files under `tools/docker/*` showing how to get up and running on Docker-slim.\n",
            "url": "https://secretagent.dev/blog/chromium-chrome",
            "title": "Moving from Chromium to Chrome",
            "date_modified": "2021-02-19T00:00:00.000Z"
        },
        {
            "id": "https://secretagent.dev/blog/handling-scale",
            "content_html": "\nWhen you start using SecretAgent, you often copy and paste the default examples. As we started to use SecretAgent on larger extraction efforts, it became clear that we didn't have a clear story for \"how\" you go from that starting example to running 2, or even 1000 scrapes.\n\nAs you start to think about structuring a bigger effort, a bunch of questions come up:\n\n- Do you create a new SecretAgent instance every time? Or do you simply add tabs?\n- How expensive is it to create many instances?\n- How should I make sure not to overload the host machine with the number of scrapes running at the same time?\n- How do I add new machines when I max out the current one?\n\nAs we explored simplifying this story, we wanted to make the progression of \"examples\" through to full-scrapes a smooth process. Something like this:\n\n#### Step 1: Try Out an Example\n\nTrying out examples should require as little setup as possible, so we added a new `default export` that's a ready-to-go client for SecretAgent.\n\n```js\nimport agent from 'secret-agent';\n\n(async () => {\n  // no initilization required!\n  await agent.goto('https://ulixee.org');\n  const datasetLinks = await agent.document.querySelectorAll('a.DatasetSummary');\n  for (const link of datasetLinks) {\n    const name = await link.querySelector('.title').textContent;\n    const href = await link.getAttribute('href');\n    const dataset = { name, href };\n    console.log('Ulixee Dataset', dataset);\n  }\n\n  await agent.close();\n})();\n```\n\n#### Step 2: Run Multiple Scrapes\n\nAgent instances are lightweight, but what do you do when you need to queue up thousands of them to run. Until now, you've been on your own to use libraries like `p-queue`, keeping track of promises, or simply waiting and looping.\n\nWe introduced a new idea into SecretAgent called a [`Handler`](/docs/basic-interfaces/handler) to help run multiple scrapes in one session. Handlers manage the concurrency of multiple scrapes to ensure your machine doesn't get overloaded and hang. We designed it so your code should require almost no changes to transition to many scrapes.\n\n```js\nimport { Handler } from 'secret-agent';\n\n(async () => {\n  const handler = new Handler({ maxConcurrency: 5 });\n\n  handler.dispatchAgent(async agent => {\n    // agent is automatically created for us\n    await agent.goto('https://ulixee.org');\n    const datasetLinks = await agent.document.querySelectorAll('a.DatasetSummary');\n    for (const link of datasetLinks) {\n      const name = await link.querySelector('.title').textContent;\n      const href = await link.getAttribute('href');\n      const dataset = { name, href };\n\n      // add a name to each agent so we can find each scrape on Replay\n      const agentOptions = { name };\n      handler.dispatchAgent(getDatasetCost, dataset, agentOptions);\n    }\n  });\n\n  // only 5 agents will be active at a given time until all are done\n  await handler.waitForAllDispatches();\n  await handler.close();\n})();\n\n// my data gets passed in once an agent is available\nasync function getDatasetCost(agent, dataset) {\n  let { name, href } = dataset;\n  if (!href.startsWith('http')) href = `https://ulixee.org${href}`;\n  console.log(href);\n  await agent.goto(href);\n  await agent.waitForPaintingStable();\n  const cost = await agent.document.querySelector('.cost .large-text').textContent;\n  console.log('Cost of %s is %s', dataset.name, cost);\n}\n```\n\n#### Step 3: Add Scraping Machines\n\nYou might find that you need to increase the speed of your scrapes. So the next transition you'll likely want to make is to add remote machines. Handlers are built to round-robin between multiple [`ConnectionToCore`](/docs/advanced/connection-to-core) instances.\n\n```js\nimport { Handler } from 'secret-agent';\n\n(async () => {\n  const handler = new Handler(\n    {\n      maxConcurrency: 5,\n      host: '192.168.1.1:2300', // fictional remote secret-agent #1\n    },\n    {\n      maxConcurrency: 5,\n      host: '192.168.1.2:2300', // fictional remote secret-agent #2\n    },\n  );\n  \n// ... everything else is the same!\n\n  handler.dispatchAgent(async agent => {\n    // agent is automatically created for us\n    await agent.goto('https://ulixee.org');\n    ...\n```\n\n\n#### Default Exports\n\nTo get to this setup, you'll notice some changes in the default exports when you install SecretAgent 1.3.0-alpha.1. The default exports that come out of the `secret-agent` and `@secret-agent/client` packages is now a pre-initialized instance of the `Agent` class (`SecretAgent` was renamed to `Agent`).\n\n[`Handler`](/docs/basic-interfaces/handler) and [`Agent`](/docs/basic-interfaces/agent) are available as exports from both the `secret-agent` and `@secret-agent/client` if you'd like to continue to use those. To customize a \"Remote\" `SecretAgent` for an [`Agent`](/docs/basic-interfaces/agent), you can create a new instance with a [`connectionToCore`](/docs/basic-interfaces/agent#constructor) parameter, or use the [`.configure()`](/docs/basic-interfaces/agent#configure) function.\n\n\n#### That's it!\n\nThat's our change. We hope it leads to a very simple model to understand how to scale up your SecretAgent instances. Feedback is welcome as always on any of our channels (listed in header)!\n",
            "url": "https://secretagent.dev/blog/handling-scale",
            "title": "Scaling SecretAgent Scrapes with Handlers",
            "date_modified": "2020-12-29T00:00:00.000Z"
        }
    ]
}